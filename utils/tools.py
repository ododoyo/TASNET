import os
import sys
import time
import logging
import librosa
import subprocess
import re
import numpy as np
import soundfile as sf
import tensorflow as tf
from scipy.io import wavfile
from tensorflow.python.training.moving_averages import assign_moving_average


def create_folders(dir_path):
    if not os.path.exists(dir_path):
        os.makedirs(dir_path)


def set_log(log_dir):
    log_path = os.path.join(log_dir, 'log.out')
    create_folders(log_dir)
    log_format = ("%(levelname)s %(asctime)s %(filename)s"
                  "[line %(lineno)d] %(message)s")
    logging.basicConfig(level=logging.DEBUG,
                        format=log_format,
                        datefmt="%Y-%m-%d %H:%M:%S",
                        filename=log_path,
                        filemode='w')


def read_raw_pcm(path, channels=1, samplerate=16000, subtype='PCM_16'):
    return sf.read(path, channels=channels, samplerate=samplerate, subtype=subtype)


def read_wav(path, offset=0.0, duration=None, samp_rate=16000):
    signal, sr = librosa.load(path, mono=False, sr=samp_rate,
                              offset=offset, duration=duration)
    return signal.astype(np.float32)


def audiowrite(path, data, samp_rate=16000):
    amp_max = max(np.abs(data))
    if amp_max > 1:
        data = data / amp_max
    data = (data + 1) / 2 * 65535 - 32768
    data = data.astype(np.int16)
    wavfile.write(path, samp_rate, data)


def get_file_line(file_path):
    line_list = []
    with open(file_path, 'r') as f:
        for line in f.readlines():
            line = line.strip().split()[0]
            line_list.append(line)
    return line_list


def samples_to_segment_len(samples, size, shift):
    # all the segments is real samples
    if samples < size:
        return 0
    else:
        return (samples - size + shift) // shift


def segment_len_to_samples(frames, size, shift):
    return frames * shift + size - shift


def segment_signal(signal, size, shift):
    seq_len = samples_to_segment_len(len(signal), size, shift)
    seg_signal = np.zeros((seq_len, size), dtype=np.float32)
    for i, j in enumerate(range(0, seq_len * shift, shift)):
        seg_signal[i] = signal[j:j + size]
    return seg_signal


def norm_signal(sig1, sig2, mix_snr, eps=1e-8):
    norm_sig1 = sig1 / np.sqrt(np.sum(sig1 ** 2) + eps)
    norm_sig2 = sig2 / np.sqrt(np.sum(sig2 ** 2) + eps)
    w = 10 ** (mix_snr / 20.0)
    norm_sig1 = norm_sig1 * w
    min_len = min(len(norm_sig1), len(norm_sig2))
    mix = norm_sig1[0:min_len] + norm_sig2[0:min_len]
    amp_max = max(np.abs(norm_sig1), np.abs(norm_sig2), np.abs(mix), eps)
    norm_sig1 = norm_sig1 * 0.9 / amp_max
    norm_sig2 = norm_sig2 * 0.9 / amp_max
    return norm_sig1, norm_sig2

def prepare_feat(s1_sig, s2_sig, size, shift):
    max_len = max(len(s1_sig), len(s2_sig))
    if max_len < size - shift:
        seq_len = 1
    else:
        seq_len = int(np.ceil((max_len - size + shift) / shift))
    sample_len = seq_len * shift + size - shift
    pad_sig1 = np.zeros(sample_len)
    pad_sig1[0: len(s1_sig)] = s1_sig
    pad_sig2 = np.zeros(sample_len)
    pad_sig2[0: len(s2_sig)] = s2_sig
    mix_sig = pad_sig1 + pad_sig2
    mix_sig = mix_sig[0: max_len]
    seg_sig1 = np.zeros((seq_len, size))
    seg_sig2 = np.zeros((seq_len, size))
    for i, j in enumerate(range(0, seq_len * shift, shift)):
        seg_sig1[i] = pad_sig1[j: j + size]
        seg_sig2[i] = pad_sig2[j: j + size]
    seg_mix = seg_sig1 + seg_sig2
    seg_src = np.stack([seg_sig1, seg_sig2], axis=0)
    return seg_mix, seg_src, seq_len, mix_sig


def get_SISNR(ref_sig, out_sig, eps=1e-8):
    assert len(ref_sig) == len(out_sig)
    ref_sig = ref_sig - np.mean(ref_sig)
    out_sig = out_sig - np.mean(out_sig)
    ref_energy = np.sum(ref_sig ** 2) + eps
    proj = np.sum(ref_sig * out_sig) * ref_sig / ref_energy
    noise = out_sig - proj
    ratio = np.sum(proj ** 2) / (np.sum(noise ** 2) + eps)
    sisnr = 10 * np.log(ratio + eps) / np.log(10.0)
    return sisnr


def getPESQ(refWav, tarWav, samp_rate):
    PESQ_path = '/home/fsl/workspace/SpeechSeparation/speech_tools/PESQ'
    fullcmd = os.path.normpath(PESQ_path) + " +" + str(samp_rate) + " " + refWav + " " + tarWav
    pesq_proc = subprocess.Popen(fullcmd, shell=True, stdout=subprocess.PIPE,
                                 universal_newlines=True)
    pesq_out = pesq_proc.communicate()

    # Parse output
    mo_pesq_out = re.compile("Prediction[^=]+=\s+([\-\d\.]+)\s*").search(pesq_out[0])
    if mo_pesq_out == None:
        print("Failed to fetch PESQ result")
        print(fullcmd)
        return -1
    return mo_pesq_out.group(1)


class MetricChecker(object):
    def __init__(self, cfg, less=True):
        self.learning_rate = cfg.learning_rate
        self.min_learning_rate = cfg.min_learning_rate
        self.decay_lr = cfg.decay_lr
        self.decay_lr_count = cfg.decay_lr_count
        self.early_stop_count = cfg.early_stop_count
        self.reset_step()
        self.cur_dev = tf.placeholder(tf.float32, shape=[], name='cur_dev')
        if not less:
            self.best_dev = tf.get_variable(name='best_dev', trainable=False, shape=[],
                                            initializer=tf.constant_initializer(-np.inf))
            self.dev_improved = tf.less(self.best_dev, self.cur_dev)
        else:
            self.best_dev = tf.get_variable(name='best_dev', trainable=False, shape=[],
                                            initializer=tf.constant_initializer(np.inf))
            self.dev_improved = tf.less(self.cur_dev, self.best_dev)
        with tf.control_dependencies([self.dev_improved]):
            if not less:
                self.update_best_dev = tf.assign(self.best_dev,
                                                 tf.maximum(self.cur_dev, self.best_dev))
            else:
                self.update_best_dev = tf.assign(self.best_dev,
                                                 tf.minimum(self.cur_dev, self.best_dev))

    def reset_step(self):
        self.stop_step = 0
        self.lr_step = 0

    def update(self, sess, cur_dev):
        dev_improved, best_dev = sess.run([self.dev_improved, self.update_best_dev],
                                          feed_dict={self.cur_dev: cur_dev})
        if dev_improved:
            self.reset_step()
        else:
            self.stop_step += 1
            self.lr_step += 1
            if self.lr_step == self.decay_lr_count:
                self.lr_step = 0
                self.learning_rate = max(self.learning_rate * self.decay_lr, self.min_learning_rate)
        return dev_improved, best_dev

    def should_stop(self):
        return self.stop_step >= self.early_stop_count

    def get_best(self, sess):
        return sess.run(self.best_dev)


def checker(cfg):
    with tf.variable_scope("De_SI-SNR_Checker"):
        snr_checker = MetricChecker(cfg, less=False)
    return snr_checker


def create_valid_summary(dev_snr):
    values = [
        tf.Summary.Value(tag='dev_SI-SNR', simple_value=dev_snr)
    ]
    summary = tf.Summary(value=values)
    return summary


def batch_norm(param, masked_param, shape, axes, offset=True, scale=True, eps=1e-6,
               decay=0.999, dtype=tf.float32, scope=None, is_train=True):
    with tf.variable_scope(scope or "BatchNorm"):
        name = param.op.name.split('/')[-1]
        running_mean = tf.get_variable('{}_running_mean'.format(name),
                                       shape=shape, initializer=zeros_init(),
                                       trainable=False, dtype=dtype)
        running_var = tf.get_variable('{}_running_var'.format(name),
                                      shape=shape, initializer=ones_init(),
                                      trainable=False, dtype=dtype)
        offset_var = None
        if offset:
            offset_var = tf.get_variable('{}_offset'.format(name),
                                         shape=shape, initializer=zeros_init(),
                                         dtype=dtype)
        scale_var = None
        if scale:
            scale_var = tf.get_variable('{}_scale'.format(name),
                                        shape=shape, initializer=ones_init(),
                                        dtype=dtype)
        def batch_statistics():
            mean = tf.reduce_mean(masked_param, axes, keep_dims=True)
            var = tf.reduce_mean(tf.square(masked_param - mean), axes, keep_dims=True)
            update_running_mean = assign_moving_average(
                running_mean, mean, decay, zero_debias=False)
            update_running_var = assign_moving_average(
                running_var, var, decay, zero_debias=False)
            with tf.control_dependencies([update_running_mean, update_running_var]):
                normed_param = tf.nn.batch_normalization(
                    param, mean, var, offset_var, scale_var, eps,
                    '{}_bn'.format(name))
            return normed_param
        def population_statistics():
            normed_param = tf.nn.batch_normalization(
                param, running_mean, running_var, offset_var, scale_var,
                eps, '{}_bn'.format(name))
            return normed_param
        normed_param = tf.cond(is_train, batch_statistics, population_statistics)
        return normed_param, running_mean, running_var


def layer_normalization(param, dims, axis=-1, offset=True, scale=True,
                        name=None, eps=1e-6, dtype=tf.float32, scope=None):
    with tf.variable_scope(scope or "LayerNorm"):
        if name is None:
            name = param.op.name.split('/')[-1]
        offset_var = 0
        if offset:
            offset_var = tf.get_variable(name+'_offset', shape=[dims],
                                         initializer=tf.zeros_initializer(),
                                         dtype=dtype)
        scale_var = 1
        if scale:
            scale_var = tf.get_variable(name+'_scale', shape=[dims],
                                        initializer=tf.ones_initializer(),
                                        dtype=dtype)
        mean = tf.reduce_mean(param, axis=axis, keep_dims=True)
        inverse_stddev = tf.rsqrt(tf.reduce_mean(
            tf.square(param - mean), axis=axis, keep_dims=True) + eps)
        normed = (param - mean) * inverse_stddev
        return normed * scale_var + offset_var


def average_gradients(tower_grads, clip_grad):
    average_grads = []
    for grad_and_vars  in zip(*tower_grads):
        grads = []
        for g, _ in grad_and_vars:
            expand_g = tf.expand_dims(g, axis=0)
            grads.append(expand_g)
        grad = tf.concat(grads, axis=0)
        grad = tf.reduce_mean(grad, axis=0)
        grad = tf.clip_by_norm(grad, clip_grad)
        v = grad_and_vars[0][1]
        grad_and_var = (grad, v)
        average_grads.append(grad_and_var)
    return average_grads
